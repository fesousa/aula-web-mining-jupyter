{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exemplo de como fazer um crawler utilizando urllib e BeatifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importações necessárias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importações\n",
    "from urllib.request import urlopen\n",
    "from urllib.parse import urljoin\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capturar conteúdo HTML da página (GET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fazer GET da página web\n",
    "init = \"https://pt.wikipedia.org/wiki/Cerveja\"\n",
    "get = urlopen(init)\n",
    "# recuperar conteúdo HTML\n",
    "html = get.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mostrar html\n",
    "html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procurar por links (marcador <a\\>) no conteúdo da página"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criar objeto do BS\n",
    "bsObj = BeautifulSoup(html, \"html.parser\")\n",
    "# encontrar todos os links <a>\n",
    "links_a = bsObj.find_all('a')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mostral links extraídos\n",
    "links_a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procurar apenas links com href válidos (não vazios e que não comecem com #)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilizar beatiful soup para pegar apenas os links válidos\n",
    "# função para verificar se o link é válido (não vazio e não inicia com #)\n",
    "def links_validos(href):\n",
    "    return href and href[0] != '#'\n",
    "# função é enviada como valor do parâmtero href\n",
    "validos = bsObj.find_all('a', href=links_validos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outros exemplos de uso do find_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# procurar links com filtro do atributo class\n",
    "filtro_classe = bsObj.find_all('a', attrs={'class':'mw-redirect'})\n",
    "filtro_classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# procurar links com filtro do atributo class e title\n",
    "filtro_classe_titulo = bsObj.find_all('a', attrs={'class':'mw-redirect', 'title':'Pilsen'})\n",
    "filtro_classe_titulo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# procurar links com recursive = False\n",
    "filtro_recursive = bsObj.find_all('a', recursive=False)\n",
    "filtro_recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# procurar links com texto Pilsen\n",
    "filtro_string = bsObj.find_all('a', string='Pilsen')\n",
    "filtro_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# procurar links utilizando **kwargs\n",
    "filtro_classe = bsObj.find_all('a', class_='mw-redirect')\n",
    "filtro_classe_titulo = bsObj.find_all('a', class_='mw-redirect', title='Pilsen')\n",
    "filtro_classe, filtro_classe_titulo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# procurar por elementos do tipo span com classe mw-headline\n",
    "filtro_span = bsObj.find_all('span', class_='mw-headline')\n",
    "# procurar por qualquer elemento que contenha o texto cerveja\n",
    "# retorna apenas o texto, e não o elemento\n",
    "import re\n",
    "filtro_cerveja = bsObj.find_all(string=re.compile('cerveja',re.IGNORECASE))\n",
    "filtro_span, filtro_cerveja"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extrair os atributos href dos links HTML e transformar nos endereços completos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retornar apenas atributo href (onde está o link)\n",
    "# e transformar no endereço completo utilizando urljoin \n",
    "# urljoin mescla dois links\n",
    "# se for um link para outro endereço, utiliza apenas o novo link\n",
    "# se for um link para o mesmo endereço, junta o endereço base com o link\n",
    "# exemplo: urljoin(\"https://pt.wikipedia.org/wiki/Cerveja\", \"/wiki/Bebida\") --> https://pt.wikipedia.org/wiki/Bebida\n",
    "# exemplo: urljoin(\"https://pt.wikipedia.org/wiki/Cerveja\", \"http://google.com\") --> http://google.com\n",
    "enderecos = [urljoin(init, l.get('href')) for l in links_a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mostrar endereços dos links\n",
    "enderecos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apenas endereços que comecem com https://pt.wikipedia.org/wiki/\n",
    "enderecos_wiki = [e for e in enderecos if e.startswith('https://pt.wikipedia.org/wiki')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enderecos_wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percorrer os 10 primeiros links para fazer fazer o GET e adicionar em uma lista\n",
    "lista_html = []\n",
    "for e in enderecos_wiki[:10]:\n",
    "    # retorna um objeto HTTPResponse\n",
    "    get = urlopen(str(e))\n",
    "    # retorna o conteudo html\n",
    "    html = get.read()\n",
    "    # coloca em uma lista\n",
    "    lista_html.append(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantidade de páginas retornadas\n",
    "lista_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# salvar html em um arquivos\n",
    "for i,h in enumerate(lista_html):\n",
    "    with open('pagina_'+str(i)+'.txt','wb') as file:\n",
    "        file.write(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Juntando tudo: crawler para recuperar páginas da wikipedia com tamanho máximo de páginas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# função para encontrar os links\n",
    "def busca_links(conteudo, link):\n",
    "    links = BeautifulSoup(conteudo, \"html.parser\").find_all('a', href=links_validos)\n",
    "    # retorna o link absoluto\n",
    "    links = [wiki for wiki in [urljoin(init, l.get('href')) for l in links] if wiki.startswith('https://pt.wikipedia.org/wiki') ]\n",
    "    print(links)\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# função para salvar link em um arquivo\n",
    "def salva_arquivo(nome, conteudo):\n",
    "    with open('wikis/'+nome,'wb') as file:\n",
    "        file.write(conteudo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# função para fazer o get\n",
    "def busca_pagina(init, max = 10):\n",
    "    buscar = set([init])\n",
    "    buscados = set()\n",
    "    contador = 1\n",
    "    while buscar and contador <= max:\n",
    "        # remove último elemento do vetor\n",
    "        link = buscar.pop()\n",
    "        if link not in buscados:\n",
    "            print(link)\n",
    "            try:                \n",
    "                \n",
    "                conteudo = urlopen(link).read()\n",
    "                salva_arquivo('wiki'+str(contador)+'.txt', conteudo)\n",
    "                contador += 1            \n",
    "                novos_links = busca_links(conteudo, link)\n",
    "                # atualiza lista para buscar com novos links.\n",
    "                # o método update do set desconsidera os repetidos os repetidos\n",
    "                buscar.update(novos_links)\n",
    "                buscados.add(link)\n",
    "            except Exception as ex:\n",
    "                print(ex)\n",
    "                pass\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# executar crawler\n",
    "init = 'https://pt.wikipedia.org/wiki/Cerveja'\n",
    "r = busca_pagina(init, max = 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
